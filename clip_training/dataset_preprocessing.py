#!/usr/bin/env python3

# script for preprocessing datasets for CLIP training
# input format: plain directory of images and txt file with same basename as image files
# output format: jsonl file with {"image": image_path, "text": caption}, images are hardlinked/copied/reflinked to output directory
#
# processing includes:
# - checking if captions fit CLIP tokenization requirements (75 tokens max)
# - chopping captions that are too long into multiple entries (and don't break comma-separated "tags")
# - skipping images without captions

import os
import tqdm
import click
import shutil
import reflink
from transformers import CLIPProcessor # for trial tokenization
from jsonlines import jsonlines
from multiprocessing import Pool, cpu_count

from clip_image_deduper.db_processing import walk_directory_relative

default_clip_model = "laion/CLIP-ViT-L-14-laion2B-s32B-b82K"
max_clip_tokens = 75

file_op_modes = ["copy", "hardlink", "reflink", "symlink"]

def process_image_caption(args):
    image_path, caption, processor = args
    # Tokenize caption
    tokens = processor.tokenizer(
        caption,
        truncation=False,
        padding=False,
        return_tensors="pt"
    )["input_ids"][0]

    # replace pathsep in path to flatten directory structure
    image_path = image_path.replace(os.path.sep, "_")

    if len(tokens) <= max_clip_tokens:
        return [{"file_name": image_path, "text": caption}]
    else:
        # Split caption into smaller parts
        parts = []
        current_part = []
        current_length = 0

        for word in caption.split(', '):
            word_tokens = processor.tokenizer(
                word,
                truncation=False,
                padding=False,
                return_tensors="pt"
            )["input_ids"][0]
            word_length = len(word_tokens)

            if current_length + word_length + 1 <= max_clip_tokens:
                current_part.append(word)
                current_length += word_length + 1  # +1 for space and comma
            else:
                parts.append(", ".join(current_part))
                current_part = [word]
                current_length = word_length

        if current_part:
            parts.append(", ".join(current_part))

        return [{"file_name": image_path, "text": part} for part in parts]

@click.command()
@click.argument("input_dataset_dir", type=click.Path(exists=True, file_okay=False, dir_okay=True))
@click.argument("output_dir", type=click.Path(file_okay=False, dir_okay=True))
@click.option("--clip-model", type=str, default=default_clip_model, help="CLIP model to use for tokenization", show_default=True)
@click.option("--mode", type=click.Choice(file_op_modes), default="copy", help="File operation mode for handling images", show_default=True)
def __main__(input_dataset_dir, output_dir, clip_model, mode):
    metadata_output_path = os.path.join(output_dir, "metadata.jsonl")
    image_output_dir = os.path.join(output_dir, "images")
    os.makedirs(image_output_dir, exist_ok=True)
    os.makedirs(output_dir, exist_ok=True)
 
    processor = CLIPProcessor.from_pretrained(clip_model)
    args_list = []
    for image_rel_path in walk_directory_relative(input_dataset_dir):
        image_base, file_ext = os.path.splitext(image_rel_path)
        if file_ext.lower() == ".txt":
            continue
        caption_path = os.path.join(input_dataset_dir, image_base + ".txt")

        if os.path.isfile(caption_path):
            with open(caption_path, "r", encoding="utf-8") as f:
                caption = f.read().strip()
            args_list.append((image_rel_path, caption, processor))
    
    results = []
    print(f"Processing {len(args_list)} image captions...")
    with Pool(cpu_count()) as pool:
        for res in tqdm.tqdm(pool.imap_unordered(process_image_caption, args_list), total=len(args_list)):
            results.extend(res)
    
    with jsonlines.open(metadata_output_path, mode="w") as writer:
        for item in results:
            writer.write(item)
    
    print(f"Processed {len(results)} caption entries, output written to {metadata_output_path}")
    print(f"Images should be placed in {image_output_dir} using '{mode}' mode.")
    for a in tqdm.tqdm(args_list):
        image_rel_path = a[0]
        src_path = os.path.join(input_dataset_dir, image_rel_path)
        # flattened image path
        dest_image_fn = image_rel_path.replace(os.path.sep, "_")
        dest_image_path = os.path.join(image_output_dir, dest_image_fn)
        if mode == "copy":
            shutil.copy2(src_path, dest_image_path)
        elif mode == "hardlink":
            os.link(src_path, dest_image_path)
        elif mode == "reflink":
            reflink.reflink(src_path, dest_image_path)
        elif mode == "symlink":
            os.symlink(src_path, dest_image_path)

if __name__ == "__main__":
    __main__()